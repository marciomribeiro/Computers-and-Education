\section{Related Work}

\label{sec:related}

Previous studies introduce aptitude tests to check whether a student will succeed or fail programming courses. To do so, some studies rely on past academic achievements. For example, Butcher et al.~\cite{butcher-predictor-high-school-1985} use high school data and ACT scores to predict college performance in computer science courses. However, their goal is to predict students who had successfully completed one year of study in computer science in general and not only in computer programming. Besides, they use data that are only available at the United States, then it is difficult to replicate to other countries. Similarly, Hostetler~\cite{hostetler-aptitude-1983} uses the college grade point average and math background to perform the predictions.

Another category of predictors rely on surveys to better understand the student skills. Hughes et al.~\cite{ibm-aptitude-test} introduce a survey based on many math and logical questions. Simon et al.~\cite{simon-predictors-ace2006} describe a multi-institutional study to predict success in introductory programming courses. To do so, they set a number of diagnostic tasks (such as map sketching and phone book searching) and perform a qualitative analysis based on short interviews. Dehnadi et al.~\cite{camel-2006} observe the mental models that students use when reasoning about simple assignments and sequence of assignments. Rountree et. al.~\cite{nathan-2004} consider factors such as student expectations of success, science background, and math background. Besides previous grades, Bergin~\cite{susan-sigce2005} consider other factors such as number of hours per week working at a part-time job and work-style preference (alone or as part of a group).

Although the surveys present promising results, applying and replicating these studies is difficult and time consuming, specially when considering large class sizes. Controlled environments that require instructors also suffer of this problem~\cite{barker-predictor-83}. Differently, the strengths of our study include an automatic predictor of students candidates to fail. Because it is automatic, it can significantly reduce the effort of professors interested in predicting such candidates. Despite achieving \higherPrecision of precision for the population, we focus only on one programming language and we use the same professor in all 7 courses at the same university, differently from Simon et al.~\cite{simon-predictors-ace2006} that use data from 11 institutions from 3 countries.

Lorenzen et al.~\cite{lorenzenC06-mastermind-predictor-sigcse2008} present a game-based predictor. They claim it is possible to predict failing students by only making them to play the game (named MasterMind). Unfortunately, the given webpage in the paper is not available anymore, so it is hard to set and replicate the study. Besides, the game does not give any clues to professors and mentors regarding the deficiencies of each student. Online judges, on the other hand, can help on this task, since it is possible to map particular subjects from the curriculum to each problem available in the judge. So, professors can identify that a particular set of students have difficulties in loops, for instance.

Another predictor consists of introducing an assembly-based language designed in a such way that students do not need to have any programming background~\cite{harris-assembly-jcsc2014}. This study correlates the results of tasks implemented in such a language with the midterm exam. They validate their study with 23 students and find a good correlation between the tasks and the midterm exam. In contrast, our evaluation focuses on the first 30 days (\semesterPercentage of the course) and use 7 semesters, totalling \totalStudents students. Therefore, we are able to predict earlier, giving better chances for professors and mentors to recover the students.

%Like the MasterMind game~\cite{lorenzenC06-mastermind-predictor-sigcse2008}, the given webpage in the paper is no longer available (despite being a very recent work), which makes the replication of the study more difficult.

To record the students actions, several approaches use an extension for the BlueJ IDE. Watson et. al.~\cite{watson-icalt-2013} propose an automatic predictor based upon how a student responds to different types of compilation errors compared to their peers. They apply the approach by using 45 students. The results shows that, after approximately 45 days, the predictor accuracy is around 60\%. Instead of compilation logs, we evaluate problem solving skills in 30 days. Another difference is that we use online judge systems, so we evaluate the student not only at the laboratory in the university.  Emily et. al.~\cite{emily-up-2008, emily-icer-2011} also use the BlueJ extension. They analyze the correlation between a metric based on syntax errors and the midterm grade. They achieve moderate correlation. Our strategy complements these studies. Therefore, we may achieve even better results by combining both approaches.

Other predictors combines the logs from the IDE with human observations~\cite{diane-acii-2011, rodrigo-behavioral-ITiCSE2009}. Differently from our work, the approaches are not automatic, focuses on the midterm exams (which might be late), and does not correlate with success and fail.

%~\cite{emily-icer-2011} midterm exam. The derived models could not predict the students.

%~\cite{marques-ia-2013} data mining. At school.