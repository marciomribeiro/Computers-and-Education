\section{Introduction}

Introductory programming courses are often perceived by the students as problematic~\cite{yadin-inroads-acm-11}. This understanding seems to be based on the high dropout rates~\cite{camel-2006, what-works-cacm-2013}, which has been observed in universities around the world, such as at the United States~\cite{bennedsen-sigcse-failure-rates-2007}, Finland~\cite{why-dropout-icer06}, and ours in Brazil.

To predict the students likely to succeed or fail, previous studies provide aptitude tests regarding programming activities. These studies focus on qualitatively identifying the student skills and consider many different variables from big, complicated, and time-consuming surveys. Also, they might bring subjectiveness due to the interview process. To perform the predictions, they use past academic achievements~\cite{hostetler-aptitude-1983, butcher-predictor-high-school-1985}, diagnostic and math/logic-based tasks~\cite{simon-predictors-ace2006, ibm-aptitude-test}, mental models~\cite{camel-2006}, games~\cite{lorenzenC06-mastermind-predictor-sigcse2008}, and even programming languages~\cite{harris-assembly-jcsc2014}. Thus, although we can predict the potential failing students, these studies do not scale: setting, executing, and replicating them is hard, time consuming, and require extra effort from the professors. To avoid this problem, automatic approaches have been proposed~\cite{watson-icalt-2013, emily-icer-2011}. However, they predict the failing students either too late or with moderate precision.

In this context, we still lack an automatic approach capable of predicting the set of students that will fail. This way, professors and mentors would be able to act and consequently help them. To achieve promising results, however, this approach must accomplish three main requirements. First, it must predict \textit{as soon as possible}, otherwise (i) there will be not enough time to act and the student would drop out the course anyway; and (ii) due to the strong prerequisites of understanding previous classes to understand the current one in programming courses (e.g., to understand loops, students must understand conditional structures), the situation gets worse, even when acting just a bit late. Second, it must predict with \textit{high precision}, otherwise professors would spend time helping many students that actually do not need much help. Third, the approach must be \textit{automatic}, requiring almost no effort from professors and mentors and allowing them to use it in every course they teach.

To accomplish these three requirements, in this article we propose an automatic strategy to early predict failing students in introductory programming courses. Our strategy complements previous approaches and consists of three simple steps. The first one is to make students use an online judge system. This kind of system executes an online submitted solution to a given problem against a set of predefined test cases to check whether the solution is correct or not. Then, we collect metrics of each student by using such a system. Finally, we execute a clustering algorithm~\cite{hartigan-clustering-algorithms-1975} to form groups so that we are able to separate the potential failing students from the other ones.

To evaluate our strategy, we conduct an empirical study regarding \semesters introductory programming courses---3.5 years, from 2010.02 to 2013.02, yielding \semesters semesters---with, in total, \totalStudents freshmen students (\studentsCoursesAverage per course, on average). The courses focused on the C language and have been given by the same professor at the Federal University of Alagoas in Brazil. We apply our strategy considering the first 30 days of the programming course (\semesterPercentage of each semester). The results suggest that our strategy can early predict the majority of the failing students within only 30 days. In particular, from the group of students our strategy points as ``likely to fail,'' \higherPrecision of the students indeed fail with 95\% standard confidence level. Moreover, we observe that the false positives our strategy points is still an important set to take into account, since at around 50\% of these students have difficulties to pass and reach the final exam. So, although they pass, this set has good candidates that need special attention as well. Also, we conclude that the efficacy of our strategy can strongly depend on the number of students in the course. Because the strategy is automatic, the effort to predict the potential failing students in a course with dozens or hundreds~\cite{bennedsen-sigcse-failure-rates-2007} can significantly reduce. In case the course has few students (e.g., 10 students), we report that our strategy adds little, since the own professor can identify the failing candidates.

We idealized and executed the entire research we present in this article in 2014. This way, we report three consequences: (i) during the \semesters semesters---from 2010.02 to 2013.02---we did not intend to write this article; (ii) the professor did not use any approach to predict the failing students (minimizing bias in this context); and (iii) the students were not aware of this research by the time they were attending the courses.

In summary, this article provides the following contributions:

\begin{itemize}

	\item A strategy to early predict the potential failing students in introductory programming courses automatically (Section~\ref{sec:strategy});
	
	\item An empirical study assessing the potential of our strategy. We evaluate our strategy by using \totalStudents students from \semesters courses during 3.5 years, demonstrating significant potential (Sections~\ref{sec:evaluation} and~\ref{sec:results}).

\end{itemize}

We organize the remainder of this article as follows. Section~\ref{sec:problem} discusses in detail the problem we address in this work. Then, Section~\ref{sec:strategy} introduces our strategy and details the three steps we consider. Section~\ref{sec:evaluation} presents the evaluation we perform whereas Section~\ref{sec:results} discusses the results and findings. We then discuss the related work in Section~\ref{sec:related}. Last but not least, we present the concluding remarks in Section~\ref{sec:conclusion}.